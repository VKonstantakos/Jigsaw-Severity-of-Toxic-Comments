{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import os; os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom transformers import AutoModelForSequenceClassification, AutoModel, AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test and Validation Dataset","metadata":{}},{"cell_type":"code","source":"class Dataset:\n    \"\"\"\n    For comments_to_score.csv (the submission), gets only one comment per row.\n    \"\"\"\n\n    def __init__(self, text, tokenizer, max_len):\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long)\n        }\n\n\nclass ValidationDataset:\n    \"\"\"\n    For validation_data.csv, loads and tokenizes both less_toxic and more_toxic.\n    \"\"\"\n\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def tokenize(self, text):\n        return self.tokenizer(text, max_length=self.max_len, \n                              padding=\"max_length\", truncation=True)\n\n    def __getitem__(self, i):\n        more_toxic = self.df['more_toxic'].iloc[i]\n        less_toxic = self.df['less_toxic'].iloc[i]\n        \n        less_inputs = self.tokenize(less_toxic)\n        more_inputs = self.tokenize(more_toxic)\n\n        return {\n            \"less_input_ids\": torch.tensor(less_inputs[\"input_ids\"], dtype=torch.long),\n            \"less_attention_mask\": torch.tensor(less_inputs[\"attention_mask\"], dtype=torch.long),\n            \"more_input_ids\": torch.tensor(more_inputs[\"input_ids\"], dtype=torch.long),\n            \"more_attention_mask\": torch.tensor(more_inputs[\"attention_mask\"], dtype=torch.long),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation\n","metadata":{}},{"cell_type":"code","source":"def validate(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n\n    df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n\n    dataset = ValidationDataset(df=df, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=64, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    n_samples = len(dataset)\n    hits = 0\n\n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n\n            less_output = model(input_ids=data['less_input_ids'], \n                                attention_mask=data['less_attention_mask'])\n\n            more_output = model(input_ids=data['more_input_ids'], \n                                attention_mask=data['more_attention_mask'])\n\n            if is_multioutput:\n                # Sum the logits of the 6 toxic labels\n                less_score = less_output.logits.sum(dim=1)\n                more_score = more_output.logits.sum(dim=1)\n                hits += (less_score < more_score).sum().item()\n            else:\n                less_score = less_output.logits[:, 1]\n                more_score = more_output.logits[:, 1]\n                hits += (less_score < more_score).sum().item()\n\n\n    accuracy = hits / n_samples\n    print(f\"Validation Accuracy: {accuracy:4.2f}\")\n\n    torch.cuda.empty_cache()\n    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the performance of HF models on validation set\n\nBest models are:\n \n- Toxic BERT\n- BERT Jigsaw\n- Toxic detector Distil-RoBERTa","metadata":{}},{"cell_type":"code","source":"# MODEL_PATH = \"../input/toxic-bert\" # 0.7058\nMODEL_PATH = '../input/hugging-face-models/toxic-detector-distilroberta' # 0.6952\n# https://huggingface.co/jpcorb20/toxic-detector-distilroberta 0.6952\n\nMAX_LENGTH = 192\nIS_MULTIOUTPUT = True\n\n# https://huggingface.co/Cameron/BERT-Jigsaw 0.6952\n# https://huggingface.co/abhishek/autonlp-toxic-new-30516963 0.6864\n# https://huggingface.co/SkolkovoInstitute/roberta_toxicity_classifier_v1 0.686\n# https://huggingface.co/unitary/unbiased-toxic-roberta 0.6848\n\n# MODEL_PATH = \"../input/roberta-base-toxicity\" # 0.6616\n# MODEL_PATH = \"../input/roberta-toxicity-classifier\" # 0.6858\n\n# MODEL_PATH = '../input/hugging-face-models/BERT-Jigsaw' # 0.6952\n\n# IS_MULTIOUTPUT = False\n# DO_VALIDATE = False\n# VALIDATION_SIZE = 5000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate(MODEL_PATH, MAX_LENGTH, IS_MULTIOUTPUT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    \n    dataset = Dataset(text=df.text.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            \n            if is_multioutput:\n                # Sum the logits for all the toxic labels\n                # One strategy out of various possible\n                output = output.logits.sum(dim=1)\n            else:\n                # Classifier. Get logits for \"toxic\"\n                output = output.logits[:, 1]\n            \n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get predictions from the best models","metadata":{}},{"cell_type":"code","source":"preds1 = generate_predictions(\"../input/toxic-bert\", max_len=192, is_multioutput=True)\npreds2 = generate_predictions(\"../input/hugging-face-models/toxic-detector-distilroberta\", max_len=192, is_multioutput=True)\npreds3 = generate_predictions(\"../input/hugging-face-models/BERT-Jigsaw\", max_len=192, is_multioutput=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble\n\nAverage of the three models.","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub[\"score_bert\"] = preds1\ndf_sub[\"score_distilrob\"] = preds2\ndf_sub[\"score_bertjig\"] = preds3\n\n# Since their scales are off, first MinMaxScale the results (per model) and then average the scores.\nsc = MinMaxScaler()\ndf_sub[[\"score_bert\", \"score_distilrob\", \"score_bertjig\"]] = sc.fit_transform(df_sub[[\"score_bert\", \"score_distilrob\", \"score_bertjig\"]])\n\ndf_sub[\"score\"] = df_sub[[\"score_bert\", \"score_distilrob\", \"score_bertjig\"]].mean(axis=1)\n\nprint(df_sub.duplicated('score').value_counts())\n\ndf_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## View some results","metadata":{}},{"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", 500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.sort_values(\"score\").head(3)[['score', 'text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.sort_values(\"score\").tail(3)[['score', 'text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit","metadata":{}},{"cell_type":"code","source":"# Tie-break, if any\ndf_sub['score'] = df_sub['score'].rank(method='first')\n\ndf_sub = df_sub[[\"comment_id\", \"score\"]]\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}