{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TFIDF_Linear_simple_baseline","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\n\nimport gc \nimport re \nimport scipy\nfrom scipy import sparse\n\nfrom pprint import pprint\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt \n\nimport time\nimport warnings\nimport scipy.optimize as optimize\n\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge, LinearRegression, RidgeCV, ElasticNetCV, LassoCV, BayesianRidge, HuberRegressor, PassiveAggressiveRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a score that measure how much toxic is a comment\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\n# Not good\n# cat_mtpl = {'toxic': 1.0, 'severe_toxic': 2.5, 'obscene': 1.0,\n#             'threat': 2.0, 'insult': 1.5, 'identity_hate': 2.0}\n\n# cat_mtpl = {'toxic': 1.0, 'severe_toxic': 2.0, 'obscene': 1.0,\n#             'threat': 1.0, 'insult': 1.0, 'identity_hate': 1.0}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train_new.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.rename(columns={'comment_text':'text'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text cleaning","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n\n    template = re.compile(r'https?://\\S+|www\\.\\S+') # Remove website links\n    text = template.sub(r'', text)\n\n    soup = BeautifulSoup(text, 'lxml') # Remove HTML tags\n    only_text = soup.get_text()\n    text = only_text\n\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n\n    text = emoji_pattern.sub(r'', text)\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n    text = re.sub(' +', ' ', text) # Remove Extra Spaces\n    text = text.strip() # Remove spaces at the beginning and at the end of string\n\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ndf_train['text'] = df_train['text'].progress_apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Undersampling","metadata":{}},{"cell_type":"code","source":"df['y'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len * 2, random_state=402)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\ndf['y'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF","metadata":{}},{"cell_type":"code","source":"vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\nX = vec.fit_transform(df['text'])\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit models","metadata":{}},{"cell_type":"code","source":"%%time\nmodel = Ridge(alpha=2.5)\nmodel.fit(X, df['y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nl_model = PassiveAggressiveRegressor(random_state=42, C=0.1, early_stopping=True)\nl_model.fit(X, df['y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ns_model = LinearSVR(C=1.0, random_state=42)\ns_model.fit(X, df['y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = Ridge(alpha=1.)\nmodel2.fit(X, df['y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nl_model2 = Ridge(alpha=2.)\nl_model2.fit(X, df['y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ns_model2 = Ridge(alpha=10.)\ns_model2.fit(X, df['y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare validation data","metadata":{}},{"cell_type":"code","source":"df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf_val.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text cleaning","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\ndf_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\ndf_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_m = model.predict(X_less_toxic)\np2_m = model.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1_m < p2_m).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_m2 = model2.predict(X_less_toxic)\np2_m2 = model2.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1_m2 < p2_m2).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_l = l_model.predict(X_less_toxic)\np2_l = l_model.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1_l < p2_l).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_l2 = l_model2.predict(X_less_toxic)\np2_l2 = l_model2.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1_l2 < p2_l2).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_s = s_model.predict(X_less_toxic)\np2_s = s_model.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1_s < p2_s).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_s2 = s_model2.predict(X_less_toxic)\np2_s2 = s_model2.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1_s2 < p2_s2).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LR models\n( (p1_m + p1_l + p1_s)/3 < (p2_m + p2_l + p2_s)/3 ).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge models\n( (p1_m2 + p1_l2 + p1_s2)/3 < (p2_m2 + p2_l2 + p2_s2)/3 ).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lsub = pd.DataFrame()\nlsub['Ridge2.5'] = p1_m\nlsub['PassiveAggressive'] = p1_l\nlsub['SVR'] = p1_s\nlsub['Ridge1'] = p1_m2\nlsub['Ridge2'] = p1_l2\nlsub['Ridge10'] = p1_s2\n\nrsub = pd.DataFrame()\nrsub['Ridge2.5'] = p2_m\nrsub['PassiveAggressive'] = p2_l\nrsub['SVR'] = p2_s\nrsub['Ridge1'] = p2_m2\nrsub['Ridge2'] = p2_l2\nrsub['Ridge10'] = p2_s2\n\n(lsub.mean(axis=1) < rsub.mean(axis=1)).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average\n( ( 0.5*(p1_m/3 + p1_l/3 + p1_s/3) + 0.5*(p1_m2/3 + p1_l2/3 + p1_s2/3) )   <   ( 0.5*(p2_m/3 + p2_l/3 + p2_s/3) + 0.5*(p2_m2/3 + p2_l2/3 + p2_s2/3) ) ).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_sub = pd.concat([lsub, rsub])\n\ncheck_l = lr_sub.rank(method='average').iloc[:30108]\ncheck_r = lr_sub.rank(method='average').iloc[30108:]\n\n(check_l.mean(axis=1) < check_r.mean(axis=1)).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare submission","metadata":{}},{"cell_type":"code","source":"base_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ntqdm.pandas()\nbase_sub['text'] = base_sub['text'].progress_apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = vec.transform(base_sub['text'])\n\np3 = model.predict(X_test)\np4 = l_model.predict(X_test)\np5 = s_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_sub['Ridge2.5'] = p3\nbase_sub['PassiveAggressive'] = p4\nbase_sub['SVR'] = p5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base_sub['score'] = (p3 + p4 + p5) / 3.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base_sub2 = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\n# tqdm.pandas()\n# base_sub2['text'] = base_sub2['text'].progress_apply(text_cleaning)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p3 = model2.predict(X_test)\np4 = l_model2.predict(X_test)\np5 = s_model2.predict(X_test)\n\nbase_sub['Ridge1'] = p3\nbase_sub['Ridge2'] = p4\nbase_sub['Ridge10'] = p5\n\n# base_sub2['score'] = (p3 + p4 + p5) / 3.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_scores = base_sub.iloc[:, -6:].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base_sub[['comment_id', 'score']].to_csv(\"base_submission.csv\", index=False)\n# base_sub2[['comment_id', 'score']].to_csv(\"base_submission2.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble of simple TF-Idf and Ridge regression\n\n### Ensemble of TfIdf - Ridge models using data from \n- Toxic competition\n- Toxic CLEANED competition\n- Ruddit toxic data\n- Toxic multilingual competition\n","metadata":{}},{"cell_type":"code","source":"def timer(func):\n    def wrapper(*args, **kws):\n        st = time.time()\n        res = func(*args, **kws)\n        et = time.time()\n        tt = (et-st)/60\n        print(f'Time taken is {tt:.2f} mins')\n        return res\n    return wrapper","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ndf_test_l = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\").replace(-1,0)\nprint(df_test.shape)\ndf_test = pd.merge(df_test, df_test_l, how=\"left\", on = \"id\")\ndf_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(df.shape)\ndf = pd.concat([df, df_test])\nprint(df.shape)\ndel df_test\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['y'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load validation data & filter for overlapping sentences","metadata":{}},{"cell_type":"code","source":"# Validation data \n\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\nprint(df_val.shape)\n\n\n# Find cases already present in toxic data\n\ndf_val = pd.merge(df_val, df.loc[:,['text']], \n                  left_on = 'less_toxic', \n                  right_on = 'text', how='left')\n\ndf_val = pd.merge(df_val, df.loc[:,['text']], \n                  left_on = 'more_toxic', \n                  right_on = 'text', how='left')\n\n# Removing those cases\ndf_val = df_val[(~df_val.text_x.isna()) | (~df_val.text_y.isna())][['worker', 'less_toxic', 'more_toxic']]\ndf_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create 3 versions of the TOXIC data","metadata":{}},{"cell_type":"code","source":"n_folds = 2\n\nfrac_1 = 0.7\nfrac_1_factor = 1.3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@timer\ndef create_folds():\n    for fld in range(n_folds):\n        print(f'Fold: {fld}')\n        tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                            df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                                random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n        tmp_df.to_csv(f'/kaggle/working/df_fld{fld}.csv', index=False)\n        print(tmp_df.shape)\n        print(tmp_df['y'].value_counts())\n\n\ncreate_folds()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create 3 versions of __clean__ TOXIC data","metadata":{}},{"cell_type":"code","source":"@timer\ndef clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    # Remove ip address\n    data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ')\n    \n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    \n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how/are/you ???\",\n                               \"hey?????\",\n                               \"hey????? 18.98.333.20 18.98.\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\"]})\ndisplay(test_clean_df)\nclean(test_clean_df,'text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = clean(df,'text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'/kaggle/working/df_clean_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df,tmp_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read toxic Ruddit data","metadata":{}},{"cell_type":"code","source":"df_ = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\nprint(df_.shape)\n\ndf_ = df_[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})\n\ndf_['y'] = (df_['y'] - df_.y.min()) / (df_.y.max() - df_.y.min()) \ndf_.y.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create 3 versions of RUDDIT data","metadata":{}},{"cell_type":"code","source":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = df_.sample(frac=frac_1, random_state = 10*(fld+1))\n    tmp_df.to_csv(f'/kaggle/working/df2_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tmp_df, df_; \ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Jigsaw multilingual data CLEANED","metadata":{}},{"cell_type":"code","source":"dfm = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\nprint(dfm.shape)\n\ndfm = clean(dfm,'comment_text')\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(dfm.loc[dfm[col]==1,['comment_text',col]].sample(5))\n    \n\n# Give more weight to severe toxic \ndfm['severe_toxic'] = dfm.severe_toxic * 2\ndfm['y'] = (dfm[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndfm['y'] = dfm['y']/dfm['y'].max()\n\ndfm = dfm[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndfm.y.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create 3 versions of Multilingual data","metadata":{}},{"cell_type":"code","source":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([dfm[dfm.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        dfm[dfm.y==0].sample(n=int(len(dfm[dfm.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, \n                                                                                        random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'/kaggle/working/dfm_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Test data  \n","metadata":{}},{"cell_type":"code","source":"# Test data\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Sklearn Pipeline with \n-  TFIDF - Take 'char_wb' as analyzer to capture subwords well\n-  Ridge - Ridge is a simple regression algorithm that will reduce overfitting ","metadata":{}},{"cell_type":"code","source":"class LengthTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[(len(x)-360)/550] for x in X])\n    def get_feature_names(self):\n        return [\"lngth\"]\n\nclass LengthUpperTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[int(sum([1 for y in x if y.isupper()])/len(x) > 0.75) ] for x in X])\n    def get_feature_names(self):\n        return [\"lngth_uppercase\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train pipeline\n\n- Load folds data\n- train pipeline\n- Predict on validation data\n- Predict on test data","metadata":{}},{"cell_type":"markdown","source":"# Training function","metadata":{}},{"cell_type":"code","source":"@timer\ndef train_pipeline(pipeline, data_path_name, n_folds, clean_prm = False):\n    val_preds_arr1_tmp = np.zeros((df_val.shape[0], n_folds))\n    val_preds_arr2_tmp = np.zeros((df_val.shape[0], n_folds))\n    test_preds_arr_tmp = np.zeros((df_sub.shape[0], n_folds))\n\n    for fld in range(n_folds):\n        print(\"\\n\\n\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n        df = pd.read_csv(f'/kaggle/working/{data_path_name}_fld{fld}.csv')\n        print(df.shape)\n\n        print(\"\\nTrain:\")\n        # Train the pipeline\n        pipeline.fit(df['text'], df['y'])\n\n        # What are the important features for toxicity\n\n        print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n        feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                      np.round(pipeline['clf'].coef_,2) )), \n                             key = lambda x:x[1], \n                             reverse=True)\n\n        display(pd.DataFrame(feature_wts[:50], columns = ['feat','val']).T)\n        #.plot('feat','val',kind='barh',figsize = (8,8) )\n        #plt.show()\n\n        if clean_prm:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline.predict(clean(df_val,'less_toxic')['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline.predict(clean(df_val,'more_toxic')['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline.predict(clean(df_sub,'text')['text'])\n        else:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline.predict(df_val['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline.predict(df_sub['text'])\n    return val_preds_arr1_tmp, val_preds_arr2_tmp, test_preds_arr_tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Toxic Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect1', LengthTransformer()),\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n    #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n        #(\"clf\",LinearRegression())\n    ]\n)\n\nval_preds_arr1, val_preds_arr2, test_preds_arr = train_pipeline(pipeline, \n                                                                \"df\", \n                                                                n_folds,\n                                                                clean_prm=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Toxic __clean__ Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1c, val_preds_arr2c, test_preds_arrc = train_pipeline(pipeline, \n                                                                   \"df_clean\", \n                                                                   n_folds,\n                                                                   clean_prm=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ruddit data Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1_, val_preds_arr2_, test_preds_arr_ = train_pipeline(pipeline, \n                                                                   \"df2\", \n                                                                   n_folds,\n                                                                   clean_prm=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mulitlingual data Training","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    #('vect1', LengthTransformer()),\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n    #(\"vect4\", CountVectorizer(min_df= 5, max_df=0.3, analyzer = 'word', ngram_range = (2,3), token_pattern=r'(?u)\\b\\w{3,}\\b', binary=True))\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n        #(\"clf\",LinearRegression())\n    ]\n)\n\nval_preds_arr1m, val_preds_arr2m, test_preds_arrm = train_pipeline(pipeline, \n                                                                   \"dfm\", \n                                                                    n_folds,\n                                                                    clean_prm=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate the pipeline ","metadata":{}},{"cell_type":"code","source":"print(\" Toxic data \")\np1 = val_preds_arr1.mean(axis=1)\np2 = val_preds_arr2.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n\nprint(\" Ruddit data \")\np3 = val_preds_arr1_.mean(axis=1)\np4 = val_preds_arr2_.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n\nprint(\" Toxic CLEAN data \")\np5 = val_preds_arr1c.mean(axis=1)\np6 = val_preds_arr2c.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')\n\nprint(\" Toxic Mulitlingual data \")\np7 = val_preds_arr1m.mean(axis=1)\np8 = val_preds_arr2m.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p7 < p8).mean() * 100,2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimize the model weights for ensemble","metadata":{}},{"cell_type":"code","source":"@timer\ndef optimize_wts():\n    func = lambda x: -1*(((x[0]*p1 + x[1]*p3 + x[2]*p5 + x[3]*p7) < (x[0]*p2 + x[1]*p4 + x[2]*p6  + x[3]*p8)).mean())\n\n    rranges = (slice(0.20, 0.6, 0.015), \n               slice(0.05, 0.5, 0.015),\n               slice(0.05, 0.5, 0.015),\n               slice(0.05, 0.5, 0.015),\n              )\n\n    resbrute = optimize.brute(func, \n                              rranges, \n                              #args=params, \n                              full_output=True,\n                              finish=None)\n    return resbrute\n\nresbrute = optimize_wts()\n\nprint(resbrute[0])  # global minimum\nprint(resbrute[1]*-1)  # function value at global minimum","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1,w2,w3,w4 = resbrute[0]\n# print(best_wts)\n\np1_wt = w1*p1 + w2*p3 + w3*p5 + w4*p7\np2_wt = w1*p2 + w2*p4 + w3*p6 + w4*p8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze bad predictions ","metadata":{}},{"cell_type":"code","source":"# Mega ridge\ndf_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\n\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\n\n(p1_wt < p2_wt).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Incorrect predictions with similar scores\n\ndf_val[(df_val.correct == 0) & (df_val.p1 < 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[(df_val.correct == 0) & (df_val.p1 > 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Some of these just look incorrectly tagged \n","metadata":{}},{"cell_type":"code","source":"### Incorrect predictions with dis-similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[(df_val.correct == 0) & (df_val['diff'] < 0.4*df_val['diff'].max())].sort_values('diff', ascending=False).head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on test data ","metadata":{}},{"cell_type":"code","source":"# Predict using pipeline\ndf_sub['score'] = w1*test_preds_arr.mean(axis=1) + \\\n                  w2*test_preds_arr_.mean(axis=1) + \\\n                  w3*test_preds_arrc.mean(axis=1) + \\\n                  w4*test_preds_arrm.mean(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correct the rank ordering","metadata":{}},{"cell_type":"code","source":"base_scores['MegaRidge'] = df_sub['score']\ncols = base_scores.columns.to_list()\n\nbase_scores.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nbase_scores2 = base_scores.copy()\n\nfor col in cols:\n    sc = MinMaxScaler()\n    base_scores2[col] = sc.fit_transform(base_scores2[col].values.reshape(-1,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub['score'] = pd.DataFrame([base_scores.rank(method='average')[c] * weights[c] for c in cols]).T.sum(axis=1).rank(method='average')\n# df_sub['score'] = base_scores.rank(method='average').mean(axis=1)\n\nweights1 = {\n    'Ridge2.5': 0.1,\n    'PassiveAggressive': 0.15,\n    'SVR': 0.15,\n    'Ridge1': 0.05,\n    'Ridge2': 0.05,\n    'Ridge10': 0.05,\n    'MegaRidge': 0.45\n}\n\nweights2 = {\n    'Ridge2.5': 0.05,\n    'PassiveAggressive': 0.2,\n    'SVR': 0.2,\n    'Ridge1': 0.05,\n    'Ridge2': 0.05,\n    'Ridge10': 0.05,\n    'MegaRidge': 0.4\n}\n\n# df_sub['score'] = pd.DataFrame([base_scores2[c] * weights1[c] for c in cols]).T.sum(axis=1).rank(method='average')\ndf_sub['score'] = base_scores2.mean(axis=1).rank(method='first')\n# df_sub['score'] = pd.DataFrame([base_scores2[c] * weights2[c] for c in cols]).T.sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_sub.duplicated('score').value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"same_score = df_sub['score'].value_counts().reset_index()[:14]\nsame_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[df_sub['score'].isin(same_score['index'].tolist())]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create average solution","metadata":{}},{"cell_type":"code","source":"# sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n# sub['score'] = 0.5*(0.5*base_sub['score'] + 0.5*base_sub2['score']) + 0.5*df_sub['score']\n# sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}