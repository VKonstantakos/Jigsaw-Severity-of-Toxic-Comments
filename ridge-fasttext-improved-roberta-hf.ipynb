{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Base Ridge Ensemble","metadata":{"papermill":{"duration":0.042932,"end_time":"2021-12-19T09:15:15.064179","exception":false,"start_time":"2021-12-19T09:15:15.021247","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom pprint import pprint\nfrom nltk.corpus import stopwords\nfrom IPython.display import display\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings; warnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']: display(df.loc[df[col] == 1, ['comment_text', col]].sample(10))\n\ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis = 1)).astype(int)\ndf['y'] = df['y'] / df['y'].max()\ndf = df[['comment_text', 'y']].rename(columns = {'comment_text': 'text'})\n\nn_folds = 7\nfrac_1 = 0.4\nfrac_1_factor = 1.5\nfor fld in range(n_folds):\n    tmp_df = pd.concat([df[df.y > 0].sample(frac = frac_1, random_state = 10 * (fld + 1)), df[df.y == 0].sample(n = int(len(df[df.y > 0]) * frac_1 * frac_1_factor), random_state = 10 * (fld + 1))], axis = 0).sample(frac = 1, random_state = 10 * (fld + 1))\n    tmp_df.to_csv(f'/kaggle/working/df_fld{fld}.csv', index = False)\n\nstop = stopwords.words('english')\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text): return [lemmatizer.lemmatize(w) for w in text]\n\ndef clean(data, col):\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)', r'\\1 \\2 \\3')\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}', r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'([*!?\\']+)', r' \\1 ')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b', r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B', r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ').str.strip()\n    data[col] = data[col].str.replace(r'[ ]{2,}', ' ').str.strip()\n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    return data\n\ntest_clean_df = pd.DataFrame({\"text\": [\"heyy\\n\\nkkdsfj\", \"hi   how/are/you ???\", \"hey?????\", \"noooo!!!!!!!!!   comeone !! \", \"cooooooooool     brooooooooooo  coool brooo\", \"naaaahhhhhhh\"]})\ndisplay(test_clean_df)\nclean(test_clean_df, 'text')\ndf = clean(df, 'text')\n\nn_folds = 7\nfrac_1 = 0.3\nfrac_1_factor = 1.2\nfor fld in range(n_folds):\n    tmp_df = pd.concat([df[df.y > 0].sample(frac = frac_1, random_state = 10 * (fld + 1)), df[df.y == 0].sample(n = int(len(df[df.y > 0]) * frac_1 * frac_1_factor), random_state = 10 * (fld + 1))], axis = 0).sample(frac = 1, random_state = 10 * (fld + 1))\n    tmp_df.to_csv(f'/kaggle/working/df_clean_fld{fld}.csv', index = False)\ndel df, tmp_df\ngc.collect()\n\ndf_ = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\ndf_ = df_[['txt', 'offensiveness_score']].rename(columns = {'txt': 'text', 'offensiveness_score': 'y'})\ndf_['y'] = (df_['y'] - df_.y.min()) / (df_.y.max() - df_.y.min())\n\nn_folds = 7\nfrac_1 = 0.7\nfor fld in range(n_folds):\n    tmp_df = df_.sample(frac = frac_1, random_state = 10 * (fld + 1))\n    tmp_df.to_csv(f'/kaggle/working/df2_fld{fld}.csv', index = False)\ndel tmp_df, df_\ngc.collect()\n\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\nclass LengthTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None): return self\n    def transform(self, X): return sparse.csr_matrix([[(len(x) - 360) / 550] for x in X])\n    def get_feature_names(self): return [\"lngth\"]\n\nclass LengthUpperTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None): return self\n    def transform(self, X): return sparse.csr_matrix([[sum([1 for y in x if y.isupper()]) / len(x)] for x in X])\n    def get_feature_names(self): return [\"lngth_uppercase\"]\n\ndf_val['upper_1'] = np.array(LengthUpperTransformer().transform(df_val['less_toxic']).todense()).reshape(-1, 1)\ndf_val['upper_2'] = np.array(LengthUpperTransformer().transform(df_val['more_toxic']).todense()).reshape(-1, 1)\ndf_val['upper_1'].hist(bins = 100)\ndf_val['upper_2'].hist(bins = 100)\nval_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    df = pd.read_csv(f'/kaggle/working/df_fld{fld}.csv')\n    features = FeatureUnion([(\"vect3\", TfidfVectorizer(min_df = 3, max_df = 0.5, analyzer = 'char_wb', ngram_range = (3, 5)))])\n    pipeline = Pipeline([(\"features\", features), (\"clf\", Ridge())])\n    pipeline.fit(df['text'], df['y'])\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), np.round(pipeline['clf'].coef_, 2))), key = lambda x: x[1], reverse = True)\n    pprint(feature_wts[:30])\n    val_preds_arr1[:, fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2[:, fld] = pipeline.predict(df_val['more_toxic'])\n    test_preds_arr[:, fld] = pipeline.predict(df_sub['text'])\n\nval_preds_arr1c = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2c = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arrc = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    df = pd.read_csv(f'/kaggle/working/df_clean_fld{fld}.csv')\n    features = FeatureUnion([(\"vect3\", TfidfVectorizer(min_df = 3, max_df = 0.5, analyzer = 'char_wb', ngram_range = (3, 5)))])\n    pipeline = Pipeline([(\"features\", features), (\"clf\", Ridge())])\n    pipeline.fit(df['text'], df['y'])\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), np.round(pipeline['clf'].coef_, 2))), key = lambda x: x[1], reverse = True)\n    pprint(feature_wts[:30])\n    val_preds_arr1c[:, fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2c[:, fld] = pipeline.predict(df_val['more_toxic'])\n    test_preds_arrc[:, fld] = pipeline.predict(df_sub['text'])\n\nval_preds_arr1_ = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2_ = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr_ = np.zeros((df_sub.shape[0], n_folds))\nfor fld in range(n_folds):\n    df = pd.read_csv(f'/kaggle/working/df2_fld{fld}.csv')\n    features = FeatureUnion([(\"vect3\", TfidfVectorizer(min_df = 3, max_df = 0.5, analyzer = 'char_wb', ngram_range = (3, 5)))])\n    pipeline = Pipeline([(\"features\", features), (\"clf\", Ridge())])\n    pipeline.fit(df['text'], df['y'])\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), np.round(pipeline['clf'].coef_, 2))), key = lambda x: x[1], reverse = True)\n    pprint(feature_wts[:30])\n    val_preds_arr1_[:, fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2_[:, fld] = pipeline.predict(df_val['more_toxic'])\n    test_preds_arr_[:, fld] = pipeline.predict(df_sub['text'])\n\ndel df, pipeline, feature_wts\ngc.collect()\n\np5 = val_preds_arr1c.mean(axis = 1)\np6 = val_preds_arr2c.mean(axis = 1)\np5 = val_preds_arr1c.mean(axis = 1)\np6 = val_preds_arr2c.mean(axis = 1)\np1 = val_preds_arr1.mean(axis = 1)\np2 = val_preds_arr2.mean(axis = 1)\np3 = val_preds_arr1_.mean(axis = 1)\np4 = val_preds_arr2_.mean(axis = 1)\np5 = val_preds_arr1c.mean(axis = 1)\np6 = val_preds_arr2c.mean(axis = 1)\n\nwts_acc = []\nfor i in range(30, 70, 1):\n    for j in range(0, 20, 1):\n        w1 = i / 100\n        w2 = (100 - i - j) / 100\n        w3 = (1 - w1 - w2)\n        p1_wt = w1 * p1 + w2 * p3 + w3 * p5\n        p2_wt = w1 * p2 + w2 * p4 + w3 * p6\n        wts_acc.append((w1, w2, w3, np.round((p1_wt < p2_wt).mean() * 100, 2)))\n\nw1, w2, w3, _ = sorted(wts_acc, key = lambda x: x[2], reverse = True)[0]\np1_wt = w1 * p1 + w2 * p3 + w3 * p5\np2_wt = w1 * p2 + w2 * p4 + w3 * p6\ndf_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\ndf_val[df_val.correct == 0].sort_values('diff', ascending = True).head(20)\ndf_val[df_val.correct == 0].sort_values('diff', ascending = False).head(20)\ndf_sub['score'] = w1 * test_preds_arr.mean(axis = 1) + w2 * test_preds_arr_.mean(axis = 1) + w3 * test_preds_arrc.mean(axis = 1)\ndf_sub['score'].count() - df_sub['score'].nunique()\n\nsame_score = df_sub['score'].value_counts().reset_index()[:10]\ndf_sub[df_sub['score'].isin(same_score['index'].tolist())]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RoBERTa Ensemble","metadata":{"papermill":{"duration":0.072167,"end_time":"2021-12-19T09:37:19.023651","exception":false,"start_time":"2021-12-19T09:37:18.951484","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel,AutoConfig\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n\nclass Config:\n    model_name = '../input/roberta-base'\n    learning_rate = 1e-4\n    epochs = 1\n    train_bs =32\n    valid_bs = 64\n    test_bs = 128\n    seed = 2021\n    max_length = 128\n    min_lr = 1e-7\n    scheduler = 'CosineAnnealingLR'\n    T_max  = 500\n    weight_decay = 1e-6\n    max_grad_norm = 1.0\n    num_classes = 1\n    margin = 0.5\n    n_fold = 5\n    n_accululate = 1\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    hidden_size = 768\n    num_hidden_layers = 24\n    dropout = 0.2\n\n\ntokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n\nMODEL_PATHS = [\n    '../input/robertabase5fold2-linear-256/Loss-Fold-0.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-1.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-2.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-3.bin',\n    '../input/robertabase5fold2-linear-256/Loss-Fold-4.bin'\n]\n\ndef set_seed(seed = 42):\n\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(Config.seed)\n\n\ndf = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()\n\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }\n\n\ntest_dataset = JigsawDataset(df, tokenizer, max_length=Config.max_length)\ntest_loader = DataLoader(test_dataset, batch_size=Config.test_bs, num_workers=2, shuffle=False, pin_memory=True)\n\nclass JModel(nn.Module):\n    def __init__(self, checkpoint=Config.model_name, Config=Config):\n        super(JModel, self).__init__()\n        self.checkpoint = checkpoint\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.layer_norm = nn.LayerNorm(Config.hidden_size)\n        self.dropout = nn.Dropout(Config.dropout)\n        self.dense = nn.Sequential(\n            nn.Linear(Config.hidden_size, 256),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Dropout(Config.dropout),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = self.layer_norm(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds        \n\n\n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    PREDS = []\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n\n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n\n    return PREDS\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JModel(Config.model_name)\n        model.to(Config.device)\n        model.load_state_dict(torch.load(path))\n\n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n\n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds    \n\npreds = inference(MODEL_PATHS, test_loader, Config.device)    \ndf['score'] = preds\ndf['score'] = df['score'].rank(method='first')\ndf.drop('text', axis=1, inplace=True)\ndf.to_csv(\"submission_bert.csv\", index=False)","metadata":{"papermill":{"duration":215.641159,"end_time":"2021-12-19T09:40:54.736975","exception":false,"start_time":"2021-12-19T09:37:19.095816","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RoBERTa Ensemble 2","metadata":{"papermill":{"duration":0.072167,"end_time":"2021-12-19T09:37:19.023651","exception":false,"start_time":"2021-12-19T09:37:18.951484","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nCONFIG = dict(\n    seed = 42,\n    model_name = '../input/roberta-base',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\nMODEL_PATHS = [\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-0.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-1.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-2.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-3.bin',\n    '../input/pytorch-w-b-jigsaw-starter/Loss-Fold-4.bin'\n]\n\ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }    \n\n\nclass JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n\n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n\n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    PREDS = []\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n\n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n\n    return PREDS\n\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n\n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n\n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\n\nset_seed(CONFIG['seed'])\ndf = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\npreds1 = inference(MODEL_PATHS, test_loader, CONFIG['device'])\npreds = pd.read_csv('submission_bert.csv')['score'].values\npreds = (preds-preds.min())/(preds.max()-preds.min())\npreds2 = (preds1-preds1.min())/(preds1.max()-preds1.min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling with (TFIDF Ridge & FastText Ensemble)","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom bs4 import BeautifulSoup\nfrom sklearn.linear_model import Ridge\nfrom gensim.models import KeyedVectors, FastText\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings; warnings.filterwarnings(\"ignore\")\n\nN_MODELS = 4\nEXTRA_DIM = 256\nALPHA_STEP_SIZE = 0.5\n\ndef text_cleaning(text):\n    template = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = template.sub(r'', text)\n    soup = BeautifulSoup(text, 'lxml')\n    only_text = soup.get_text()\n    text = only_text\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"\n                               u\"\\U0001F300-\\U0001F5FF\"\n                               u\"\\U0001F680-\\U0001F6FF\"\n                               u\"\\U0001F1E0-\\U0001F1FF\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text)\n    text = re.sub(' +', ' ', text)\n    text = text.strip()\n    return text\n\ndf = pd.read_csv('../input/jigsaw-regression-based-data/train_data_version2.csv')\ndf = df.dropna(axis = 0)\nvec = TfidfVectorizer(min_df = 3, max_df = 0.5, analyzer = 'char_wb', ngram_range = (3, 5), max_features = 46000)\nvec.fit(df['text'])\nfmodel = FastText.load('../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin')\n\ndef splitter(text): return [word for word in text.split(' ')]\ndef vectorizer(text):\n    tokens = splitter(text)\n    x1 = vec.transform([text]).toarray()\n    x2 = np.mean(fmodel.wv[tokens], axis = 0).reshape(1, -1)\n    x = np.concatenate([x1, x2], axis = -1).astype(np.float16)\n    del x1\n    del x2\n    return x\n\nX_np = np.array([vectorizer(text) for text in df.text]).reshape(-1, (len(vec.vocabulary_) + EXTRA_DIM))\nX = sparse.csr_matrix(X_np)\ndel X_np\n\nclass RidgeEnsemble():\n    def __init__(self, n_models = 4, alpha_step_size = 0.5): self.models = [Ridge(alpha = alpha) for alpha in [alpha_step_size * i for i in range(1, n_models + 1)]]\n    def fit(self, X, y): self.models = [model.fit(X, y) for model in self.models]\n    def predict(self, X): return np.mean(np.concatenate([np.expand_dims(model.predict(X), axis = 0) for model in self.models], axis = 0), axis = 0)\n\nmodel = RidgeEnsemble()\nmodel.fit(X, df['y'])\n\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\n\nX_less_toxic_temp = []\nfor text in df_val.less_toxic: X_less_toxic_temp.append(vectorizer(text))\nX_less_toxic_temp = np.array(X_less_toxic_temp).reshape(-1, (len(vec.vocabulary_) + EXTRA_DIM))\nX_less_toxic = sparse.csr_matrix(X_less_toxic_temp)\ndel X_less_toxic_temp\n\nX_more_toxic_temp = []\nfor text in df_val.more_toxic: X_more_toxic_temp.append(vectorizer(text))\nX_more_toxic_temp = np.array(X_more_toxic_temp).reshape(-1, (len(vec.vocabulary_) + EXTRA_DIM))\nX_more_toxic = sparse.csr_matrix(X_more_toxic_temp)\ndel X_more_toxic_temp\n\ndf_sub2 = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf_sub2['text'] = df_sub2['text'].apply(text_cleaning)\nX_sub_temp = []\nfor text in df_sub2.text: X_sub_temp.append(vectorizer(text))\nX_sub_temp = np.array(X_sub_temp).reshape(-1, (len(vec.vocabulary_) + 256))\nX_test = sparse.csr_matrix(X_sub_temp)\ndel X_sub_temp\n\ndf_sub2['score'] = model.predict(X_test)\ndf_sub2['score'] = df_sub2['score']\ndf_sub2[['comment_id', 'score']].to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os; os.environ['TOKENIZERS_PARALLELISM'] = 'false'\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoModel, AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    \"\"\"\n    For comments_to_score.csv (the submission), get only one comment per row\n    \"\"\"\n    def __init__(self, text, tokenizer, max_len):\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long)\n        }\n    \n\ndef generate_predictions(model_path, max_len, is_multioutput):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n    \n    dataset = Dataset(text=df.text.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=2, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for data in data_loader:\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            \n            if is_multioutput:\n                # Sum the logits for all the toxic labels\n                # One strategy out of various possible\n                output = output.logits.sum(dim=1)\n            else:\n                # Classifier. Get logits for \"toxic\"\n                output = output.logits[:, 1]\n            \n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds1 = generate_predictions(\"../input/toxic-bert\", max_len=192, is_multioutput=True)\npreds2 = generate_predictions(\"../input/hugging-face-models/toxic-detector-distilroberta\", max_len=192, is_multioutput=True)\npreds3 = generate_predictions(\"../input/hugging-face-models/BERT-Jigsaw\", max_len=192, is_multioutput=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nhf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\nhf_sub[\"score_bert\"] = preds1\nhf_sub[\"score_distilrob\"] = preds2\nhf_sub[\"score_bertjig\"] = preds3\n\nsc = MinMaxScaler()\nhf_sub[[\"score_bert\", \"score_distilrob\", \"score_bertjig\"]] = sc.fit_transform(hf_sub[[\"score_bert\", \"score_distilrob\", \"score_bertjig\"]])\n\nhf_sub[\"score\"] = hf_sub[[\"score_bert\", \"score_distilrob\", \"score_bertjig\"]].mean(axis=1)\n\nprint(hf_sub.duplicated('score').value_counts())\n\nhf_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'] = (0.4 * ((df_sub['score'] * 0.94) + (preds * 0.06)) ) +  (0.4 * df_sub2['score']) + (0.2 * hf_sub['score'])\n\nprint(df_sub.duplicated('score').value_counts())\n\ndf_sub['score'] = df_sub['score'].rank(method='first')\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"papermill":{"duration":0.234906,"end_time":"2021-12-19T09:40:55.693309","exception":false,"start_time":"2021-12-19T09:40:55.458403","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not is_private():\n#     df_sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n#     df_sub['score'] = 0.0\n#     df_sub.to_csv('submission.csv', index = False)","metadata":{"papermill":{"duration":0.26057,"end_time":"2021-12-19T09:40:56.182363","exception":false,"start_time":"2021-12-19T09:40:55.921793","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}